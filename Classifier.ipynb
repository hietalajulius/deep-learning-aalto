{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import getLoader, path_to_train, path_to_test,loader_batch_size_train,loader_batch_size_test, path_to_slack,loader_batch_size_slack\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths: data/large/processed_train.csv data/large/processed_test.csv data/large/slack_test.csv\n",
      "batch size train: 512\n",
      "batch size test: 5\n",
      "batch size slack: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Paths:\",path_to_train,path_to_test,path_to_slack)\n",
    "print(\"batch size train:\",loader_batch_size_train)\n",
    "print(\"batch size test:\",loader_batch_size_test)\n",
    "print(\"batch size slack:\",loader_batch_size_slack)\n",
    "skip_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dictionary_size, hidden_size, output_size=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs: Tensor [max_seq_length, batch_size, 1]\n",
    "          seq_lengths: list of sequence lengths\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "          outputs: Tensor [max_seq_length, batch_size, hidden_size]\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        batch_size = pad_seqs.shape[1]\n",
    "        \n",
    "        embedded = self.embedding(pad_seqs).view(pad_seqs.shape[0], pad_seqs.shape[1], -1)\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths, batch_first = False)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        _,hidden = self.lstm(packed)\n",
    "        \n",
    "        fc = self.linear(hidden[0])\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def init_hidden(self, batch_size=1, device=device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "dictionary_size = 20000\n",
    "classifier = Classifier(dictionary_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = getLoader(train=True,mini=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "in: torch.Size([41, 512, 1])\n",
      "tensor([[[0.3114, 0.0531],\n",
      "         [0.3121, 0.1002],\n",
      "         [0.2996, 0.0676],\n",
      "         ...,\n",
      "         [0.3280, 0.1205],\n",
      "         [0.2730, 0.0858],\n",
      "         [0.2841, 0.0595]]], grad_fn=<AddBackward0>)\n",
      "out: torch.Size([1, 512, 2])\n"
     ]
    }
   ],
   "source": [
    "#Quick output test\n",
    "for i, batch in enumerate(trainloader):\n",
    "    print(\"iter\", i)\n",
    "    pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "    batch_size = pad_input_seqs.size(1)\n",
    "    pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "    classifier_hidden = classifier.init_hidden(batch_size, device)\n",
    "    print(\"in:\",pad_input_seqs.size())\n",
    "    classifier_hidden = classifier(pad_input_seqs, input_seq_lengths, classifier_hidden)\n",
    "    print(classifier_hidden)\n",
    "    print(\"out:\",classifier_hidden.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 200 \n",
    "    print(\"Epoch\", epoch+1)\n",
    "    \n",
    "    for i, batch in enumerate(trainloader):\n",
    "        classifier_optimizer.zero_grad()\n",
    "        \n",
    "        pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "        batch_size = pad_input_seqs.size(1)\n",
    "        pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "        classifier_hidden = classifier.init_hidden(batch_size, device)\n",
    "\n",
    "        # Encode input sequence\n",
    "        classifier_hidden = classifier(pad_input_seqs, input_seq_lengths, classifier_hidden)\n",
    "\n",
    "        #print(classifier_hidden.size(),pad_target_seqs.size())\n",
    "        loss = criterion(classifier_hidden.view(batch_size,2), pad_target_seqs)\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        classifier_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1) or i == (len(trainloader) // trainloader.batch_size):\n",
    "            print('[%d, %5d] loss: %.4f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if skip_training:\n",
    "            break\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded from classifier_model.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk, submit these files together with your notebook\n",
    "classifier_filename = 'classifier_model.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        torch.save(classifier.state_dict(), classifier_filename)\n",
    "        print('Model saved to %s' % (classifier_filename))\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    classifier = Classifier(dictionary_size, hidden_size)\n",
    "    classifier.load_state_dict(torch.load(classifier_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Classifier loaded from %s.' % classifier_filename)\n",
    "    classifier = classifier.to(device)\n",
    "    classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = getLoader(train=False,mini=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every_test = 100000\n",
    "def compute_accuracy(classifier, testloader):\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for pad_input_seqs, input_seq_lengths, targets, _ in testloader:\n",
    "            batch_size = pad_input_seqs.size(1)\n",
    "            \n",
    "            pad_input_seqs, targets = pad_input_seqs.to(device), targets.to(device)\n",
    "            \n",
    "            init_hidden = classifier.init_hidden(batch_size, device)\n",
    "            output = classifier(pad_input_seqs, input_seq_lengths, init_hidden)\n",
    "            \n",
    "            out_flat = output.detach().numpy().argmax(axis=2)\n",
    "            predicted = torch.tensor(out_flat)\n",
    "            #print(\"pred\", predicted)\n",
    "            #print(\"true\", targets)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            #print(\"tot\",total)\n",
    "            #print(\"cor\",correct)\n",
    "            if (total % print_every_test == 0):\n",
    "                print(\"Counted:\",total,\"accuracy\",correct / total)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted: 20000 accuracy 0.831\n",
      "Counted: 40000 accuracy 0.82795\n",
      "Counted: 60000 accuracy 0.8256\n",
      "Counted: 80000 accuracy 0.82655\n",
      "Counted: 100000 accuracy 0.82632\n",
      "Counted: 120000 accuracy 0.8268416666666667\n",
      "Counted: 140000 accuracy 0.8270357142857143\n",
      "Counted: 160000 accuracy 0.8275625\n",
      "Counted: 180000 accuracy 0.8276388888888889\n",
      "Counted: 200000 accuracy 0.827915\n",
      "Counted: 220000 accuracy 0.8279909090909091\n",
      "Counted: 240000 accuracy 0.827875\n",
      "Counted: 260000 accuracy 0.8280961538461539\n",
      "Counted: 280000 accuracy 0.8280571428571428\n",
      "Counted: 300000 accuracy 0.8277633333333333\n",
      "Counted: 320000 accuracy 0.827578125\n",
      "0.827578125\n"
     ]
    }
   ],
   "source": [
    "print(\"Final train accuracy:\",compute_accuracy(classifier,trainloader))\n",
    "print(\"Final test accuracy:\",compute_accuracy(classifier,testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "slackloader = getLoader(train=False,mini=False, slack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.read_csv(\"data/words.csv\",squeeze=True)\n",
    "index_word = {x:y for x,y in enumerate(word_df[\"0\"])}\n",
    "word_index = {y:x for x,y in enumerate(word_df[\"0\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every_slack = 200\n",
    "def evaluate_slack(classifier, testloader):\n",
    "    classifier.eval()\n",
    "    negative = 0\n",
    "    positive = 0\n",
    "    total = 0\n",
    "    pos_scores = {}\n",
    "    neg_scores = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (pad_input_seqs, input_seq_lengths, targets , _) in enumerate(testloader):\n",
    "            batch_size = pad_input_seqs.size(1)\n",
    "            \n",
    "            pad_input_seqs = pad_input_seqs.to(device)\n",
    "            \n",
    "            init_hidden = classifier.init_hidden(batch_size, device)\n",
    "            output = classifier(pad_input_seqs, input_seq_lengths, init_hidden)\n",
    "            \n",
    "            out_flat = output.detach().numpy().argmax(axis=2)\n",
    "            predicted = torch.tensor(out_flat)\n",
    "            \n",
    "            \n",
    "            total += targets.size(0)\n",
    "            positive += (predicted == 1).sum().item()\n",
    "            negative += (predicted == 0).sum().item()\n",
    "\n",
    "            pos_scores.update({i:output.numpy().flatten()[1]})\n",
    "            neg_scores.update({i:output.numpy().flatten()[0]})\n",
    "            \n",
    "\n",
    "            if (total % print_every_slack == 0):\n",
    "                print(\"Counted:\",total,\"positive\",positive / total,\"negative\",negative / total)\n",
    "    return positive, negative, total, pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted: 200 positive 0.545 negative 0.455\n",
      "Counted: 400 positive 0.5275 negative 0.4725\n",
      "Counted: 600 positive 0.505 negative 0.495\n",
      "Counted: 800 positive 0.50125 negative 0.49875\n",
      "Counted: 1000 positive 0.504 negative 0.496\n"
     ]
    }
   ],
   "source": [
    "pos_count,neg_count,total, pos_scores, neg_scores = evaluate_slack(classifier,slackloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = sorted(pos_scores.items(), key=lambda s: -s[1])\n",
    "neg_scores = sorted(neg_scores.items(), key=lambda s: -s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "listed = list(slackloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive sentence 1\n",
      "set the channel purpose for finding project partners \n",
      "\n",
      "Top Positive sentence 2\n",
      "yes the exercise sessions . thanks ! \n",
      "\n",
      "Top Positive sentence 3\n",
      "in CTX the type torch CTX is actually float doing your CTX CTX will convert torch CTX to torch CTX \n",
      "\n",
      "Top Positive sentence 4\n",
      "and you can avoid having to CTX the CTX every time you pass a data sample by CTX the data into CTX and avoid extra CTX this way . that s why i said that a simple matrix CTX will do this trick for you if you think how matrix CTX works you can see that it exactly does this combination by adding the individual CTX for every data example . \n",
      "\n",
      "Top Positive sentence 5\n",
      "in . CTX in the return value description is returns CTX CTX CTX of the CTX shape CTX length hidden size . hidden CTX new state of the CTX shape batch size hidden size with batch size . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,top in enumerate(pos_scores[:5]):\n",
    "    print(\"Top Positive sentence\",i+1)\n",
    "    sentence = listed[top[0]]\n",
    "    for word in sentence[0].numpy().flatten():\n",
    "        if (word != 2 and word != 3 and word != 0):\n",
    "            print(index_word[word],end=\" \")\n",
    "    print(\"\\n\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Negative sentence 1\n",
      "are there any tutorial sessions this week ? the booking option is not available on CTX edited \n",
      "\n",
      "Top Negative sentence 2\n",
      "when i fetch data and click on it it says empty . i can t seem to download the data . \n",
      "\n",
      "Top Negative sentence 3\n",
      "class CTX nn CTX def init self dictionary size hidden size super CTX self . init self CTX size hidden size self CTX nn CTX dictionary size hidden size self CTX nn CTX hidden size hidden size def forward self pad CTX CTX CTX hidden CTX pad CTX CTX max CTX length batch size CTX CTX list of sequence CTX hidden CTX batch size hidden size returns CTX CTX max CTX length batch size hidden size hidden CTX batch size hidden size your code here pad CTX self CTX pad CTX packed sequence pack CTX sequence pad CTX CTX CTX batch first false input CTX packed sequence CTX in length input CTX CTX CTX collect CTX CTX at different processing steps in this list el for batch size in packed sequence CTX sizes for i in range batch size CTX input CTX el CTX CTX hidden i self CTX CTX hidden i CTX packed sequence CTX el CTX CTX CTX CTX el CTX torch CTX CTX dim packed sequence CTX CTX CTX pad packed sequence packed sequence batch first false CTX CTX CTX CTX CTX CTX CTX return CTX hidden def init hidden self batch size device device return torch CTX batch size self CTX size device device \n",
      "\n",
      "Top Negative sentence 4\n",
      "i didn t get any feedback ? \n",
      "\n",
      "Top Negative sentence 5\n",
      "i m getting this type of loss at the very start is this normal ? loss . loss . loss . loss . loss . loss . loss . loss . loss . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,top in enumerate(neg_scores[:5]):\n",
    "    print(\"Top Negative sentence\",i+1)\n",
    "    sentence = listed[top[0]]\n",
    "    for word in sentence[0].numpy().flatten():\n",
    "        if (word != 2 and word != 3 and word != 0):\n",
    "            print(index_word[word],end=\" \")\n",
    "    print(\"\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
