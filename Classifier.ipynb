{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import getLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dictionary_size, hidden_size, output_size=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs: Tensor [max_seq_length, batch_size, 1]\n",
    "          seq_lengths: list of sequence lengths\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "          outputs: Tensor [max_seq_length, batch_size, hidden_size]\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        batch_size = pad_seqs.shape[1]\n",
    "        \n",
    "        embedded = self.embedding(pad_seqs).view(pad_seqs.shape[0], pad_seqs.shape[1], -1)\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths, batch_first = False)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        _,hidden = self.lstm(packed)\n",
    "        \n",
    "        fc = self.linear(hidden[0])\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def init_hidden(self, batch_size=1, device=device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "dictionary_size = 10 #dummy\n",
    "classifier = Classifier(dictionary_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = getLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "in: torch.Size([7, 4, 1])\n",
      "tensor([[[-0.0724,  0.1268],\n",
      "         [-0.0665,  0.1390],\n",
      "         [ 0.1033,  0.0062],\n",
      "         [-0.0558,  0.2765]]], grad_fn=<AddBackward0>)\n",
      "out: torch.Size([1, 4, 2])\n",
      "iter 1\n",
      "in: torch.Size([5, 4, 1])\n",
      "tensor([[[ 0.1599,  0.0069],\n",
      "         [ 0.0333,  0.0057],\n",
      "         [ 0.0435, -0.0501],\n",
      "         [ 0.0270, -0.0480]]], grad_fn=<AddBackward0>)\n",
      "out: torch.Size([1, 4, 2])\n",
      "iter 2\n",
      "in: torch.Size([9, 4, 1])\n",
      "tensor([[[ 0.2070,  0.0440],\n",
      "         [-0.0805,  0.0787],\n",
      "         [-0.0503,  0.2583],\n",
      "         [ 0.1154, -0.0208]]], grad_fn=<AddBackward0>)\n",
      "out: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "#Quick output test\n",
    "for i, batch in enumerate(trainloader):\n",
    "    print(\"iter\", i)\n",
    "    pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "    batch_size = pad_input_seqs.size(1)\n",
    "    pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "    classifier_hidden = classifier.init_hidden(batch_size, device)\n",
    "    print(\"in:\",pad_input_seqs.size())\n",
    "    classifier_hidden = classifier(pad_input_seqs, input_seq_lengths, classifier_hidden)\n",
    "    print(classifier_hidden)\n",
    "    print(\"out:\",classifier_hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tensor([[[-0.0724,  0.1268],\n",
      "         [ 0.1033,  0.0062],\n",
      "         [ 0.1154, -0.0208],\n",
      "         [ 0.0270, -0.0480]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.7588797807693481\n",
      "tensor([[[ 0.0554,  0.0014],\n",
      "         [-0.1235,  0.1138],\n",
      "         [-0.1252,  0.1281],\n",
      "         [-0.0482,  0.2682]]], grad_fn=<AddBackward0>) tensor([1, 1, 1, 0])\n",
      "loss 0.6850789189338684\n",
      "tensor([[[ 0.0130,  0.2832],\n",
      "         [-0.1406,  0.3120],\n",
      "         [ 0.0399,  0.1481],\n",
      "         [-0.2029,  0.2378]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 1])\n",
      "loss 0.7569455504417419\n",
      "Epoch 1\n",
      "tensor([[[ 0.1690,  0.0023],\n",
      "         [-0.4103,  0.4128],\n",
      "         [ 0.1621,  0.0219],\n",
      "         [-0.2686,  0.3668]]], grad_fn=<AddBackward0>) tensor([0, 1, 0, 1])\n",
      "loss 0.5069711208343506\n",
      "tensor([[[ 0.2116,  0.0574],\n",
      "         [-0.4379,  0.4302],\n",
      "         [ 0.0178,  0.0201],\n",
      "         [-0.3841,  0.3841]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.5106432437896729\n",
      "tensor([[[ 0.1387, -0.1106],\n",
      "         [-0.0975,  0.2238],\n",
      "         [-0.4658,  0.4952],\n",
      "         [-0.2481,  0.3517]]], grad_fn=<AddBackward0>) tensor([0, 0, 1, 1])\n",
      "loss 0.5510916113853455\n",
      "Epoch 2\n",
      "tensor([[[-0.6343,  0.6276],\n",
      "         [ 0.5566, -0.4132],\n",
      "         [-0.5138,  0.6155],\n",
      "         [-0.5271,  0.5357]]], grad_fn=<AddBackward0>) tensor([1, 0, 1, 1])\n",
      "loss 0.28690987825393677\n",
      "tensor([[[-0.0282,  0.0607],\n",
      "         [-0.0378,  0.1499],\n",
      "         [-0.2686,  0.3522],\n",
      "         [-0.8244,  0.8286]]], grad_fn=<AddBackward0>) tensor([1, 0, 1, 1])\n",
      "loss 0.5116212368011475\n",
      "tensor([[[ 0.6608, -0.3996],\n",
      "         [ 0.2380, -0.2117],\n",
      "         [-0.7081,  0.7478],\n",
      "         [ 0.7260, -0.5828]]], grad_fn=<AddBackward0>) tensor([0, 0, 1, 0])\n",
      "loss 0.30986225605010986\n",
      "Epoch 3\n",
      "tensor([[[ 0.0387,  0.0755],\n",
      "         [ 0.8178, -0.6757],\n",
      "         [-1.0289,  1.0375],\n",
      "         [-0.7322,  0.8493]]], grad_fn=<AddBackward0>) tensor([0, 0, 1, 1])\n",
      "loss 0.3051507771015167\n",
      "tensor([[[ 0.9473, -0.6645],\n",
      "         [-0.2976,  0.3734],\n",
      "         [ 0.9662, -0.8150],\n",
      "         [-0.8224,  0.8484]]], grad_fn=<AddBackward0>) tensor([0, 1, 0, 1])\n",
      "loss 0.23071449995040894\n",
      "tensor([[[ 0.4708, -0.4455],\n",
      "         [-0.0726,  0.1153],\n",
      "         [-1.1371,  1.1364],\n",
      "         [-0.8973,  0.9624]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.2956911623477936\n",
      "Epoch 4\n",
      "tensor([[[ 1.2293, -0.9161],\n",
      "         [ 0.2451, -0.1006],\n",
      "         [ 1.1752, -1.0087],\n",
      "         [-0.9997,  1.0659]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 1])\n",
      "loss 0.21795770525932312\n",
      "tensor([[[-1.3608,  1.3693],\n",
      "         [-0.1431,  0.1969],\n",
      "         [-0.3798,  0.4526],\n",
      "         [ 1.1784, -1.0268]]], grad_fn=<AddBackward0>) tensor([1, 1, 1, 0])\n",
      "loss 0.2666110098361969\n",
      "tensor([[[ 0.7002, -0.6670],\n",
      "         [-1.0417,  1.2074],\n",
      "         [-1.5216,  1.5574],\n",
      "         [-1.1276,  1.1613]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.11720991134643555\n",
      "Epoch 5\n",
      "tensor([[[ 0.7983, -0.7600],\n",
      "         [-0.3732,  0.4419],\n",
      "         [-1.3092,  1.3879],\n",
      "         [-0.5499,  0.6051]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.22414380311965942\n",
      "tensor([[[-1.8668,  1.8928],\n",
      "         [ 0.2996, -0.1521],\n",
      "         [-1.2183,  1.4000],\n",
      "         [-1.7785,  1.8166]]], grad_fn=<AddBackward0>) tensor([1, 0, 1, 1])\n",
      "loss 0.15327665209770203\n",
      "tensor([[[ 1.8877, -1.4777],\n",
      "         [ 1.6233, -1.4088],\n",
      "         [ 1.4471, -1.2844],\n",
      "         [-1.3877,  1.4189]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 1])\n",
      "loss 0.05070161819458008\n",
      "Epoch 6\n",
      "tensor([[[ 0.3794, -0.2106],\n",
      "         [ 1.7125, -1.4861],\n",
      "         [-1.6873,  1.7724],\n",
      "         [-1.9972,  2.0515]]], grad_fn=<AddBackward0>) tensor([0, 0, 1, 1])\n",
      "loss 0.13232681155204773\n",
      "tensor([[[-2.2627,  2.3182],\n",
      "         [-0.9197,  1.0330],\n",
      "         [-0.9162,  0.9557],\n",
      "         [-1.5340,  1.5698]]], grad_fn=<AddBackward0>) tensor([1, 1, 1, 1])\n",
      "loss 0.08246523141860962\n",
      "tensor([[[ 2.2342, -1.7670],\n",
      "         [ 1.3925, -1.3210],\n",
      "         [ 1.6371, -1.4618],\n",
      "         [-1.4307,  1.6689]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 1])\n",
      "loss 0.042629778385162354\n",
      "Epoch 7\n",
      "tensor([[[ 1.5201, -1.4424],\n",
      "         [ 0.7556, -0.5083],\n",
      "         [ 1.9801, -1.7169],\n",
      "         [ 1.6979, -1.5179]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 0])\n",
      "loss 0.09076948463916779\n",
      "tensor([[[-2.6010,  2.6943],\n",
      "         [-1.2252,  1.3885],\n",
      "         [-1.8589,  2.0088],\n",
      "         [-2.3097,  2.4249]]], grad_fn=<AddBackward0>) tensor([1, 1, 1, 1])\n",
      "loss 0.02628573775291443\n",
      "tensor([[[ 2.5456, -2.0273],\n",
      "         [-1.1534,  1.2153],\n",
      "         [-1.5196,  1.8033],\n",
      "         [-1.8092,  1.8558]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.040111392736434937\n",
      "Epoch 8\n",
      "tensor([[[ 2.6347, -2.1036],\n",
      "         [-1.4293,  1.6239],\n",
      "         [-2.8091,  2.9299],\n",
      "         [-1.9353,  2.1142]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 1])\n",
      "loss 0.018833816051483154\n",
      "tensor([[[ 2.0072, -1.9099],\n",
      "         [-2.5469,  2.7009],\n",
      "         [ 1.9349, -1.7346],\n",
      "         [-1.9464,  1.9965]]], grad_fn=<AddBackward0>) tensor([0, 1, 0, 1])\n",
      "loss 0.01733154058456421\n",
      "tensor([[[ 1.4132, -1.0532],\n",
      "         [ 2.4011, -2.0809],\n",
      "         [-1.4473,  1.5032],\n",
      "         [-1.6516,  1.9658]]], grad_fn=<AddBackward0>) tensor([0, 0, 1, 1])\n",
      "loss 0.042553603649139404\n",
      "Epoch 9\n",
      "tensor([[[-1.8501,  2.0704],\n",
      "         [-1.5596,  1.6119],\n",
      "         [-2.7077,  2.8845],\n",
      "         [-1.6905,  2.0159]]], grad_fn=<AddBackward0>) tensor([1, 1, 1, 1])\n",
      "loss 0.022177159786224365\n",
      "tensor([[[ 2.2883, -2.1810],\n",
      "         [ 1.5919, -1.2057],\n",
      "         [ 2.5433, -2.2024],\n",
      "         [-2.1455,  2.2000]]], grad_fn=<AddBackward0>) tensor([0, 0, 0, 1])\n",
      "loss 0.023023754358291626\n",
      "tensor([[[ 2.9998, -2.4227],\n",
      "         [-3.3648,  3.5499],\n",
      "         [-2.2174,  2.4432],\n",
      "         [ 2.1298, -1.9086]]], grad_fn=<AddBackward0>) tensor([0, 1, 1, 0])\n",
      "loss 0.008071959018707275\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    for i, batch in enumerate(trainloader):\n",
    "        classifier_optimizer.zero_grad()\n",
    "        \n",
    "        pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "        batch_size = pad_input_seqs.size(1)\n",
    "        pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "        classifier_hidden = classifier.init_hidden(batch_size, device)\n",
    "\n",
    "        # Encode input sequence\n",
    "        classifier_hidden = classifier(pad_input_seqs, input_seq_lengths, classifier_hidden)\n",
    "\n",
    "        print(classifier_hidden,pad_target_seqs)\n",
    "        loss = criterion(classifier_hidden.view(4,2), pad_target_seqs)\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        classifier_optimizer.step()\n",
    "        print(\"loss\",loss.item())\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
