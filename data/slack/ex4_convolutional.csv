0
"""5x5 kernel (the input of the layer should be 5x5)"" can someone elaborate on what this means, thanks"
"@Alexander Ilin I trained ResNet without normalizing the input values and achieved 0.911 accuracy on test images, compared to 0.908 with normalized images. I'm wondering why one would make the choice to normalize the image values, when the result can be equally good or even better without it?"
"And the last question, should the 1x1 conv in skip connection have bias term?"
Are there any tutorial sessions this week? The booking option is not available on mycourses (edited) 
Did jupyter.cs.aalto.fi just crach? 502 bad gateway it says. I noticed that can't log into kosh or aalto webmail either... DOS going on?
Especially when the network definition is basically exactly the same as the reference network on the autograder side?
"For VGG, do we use ReLU after each layer in a block, or only after each block? I have consulted the Very Deep Convolutional Networks for Large-Scale Image Recognition paper and it simply says: ""The ReLU activation function is not shown for brevity."""
Hey @Alexander Ilin shouldn’t the feedback for round 4 have 13 points as said in the first lecture? Mine has only 10 total points. (edited) 
"Hi, I have one question about vgg. The channel parameter can be hardcoded? or not?"
"Hi, I used OrderedDict and sequential and everything worked fine in Colab but somehow the autograder removed these and I lost points because of that."
"Hi, I'm getting size mismatch error in the 41_letnet exercise. The error comes from when the networks moves to the first fully-connected layer. I'm using 32*7*7 as the amount of input channels, is there something wrong with this?"
"Hi, could you please comment on my question from days ago? I'll make it more clear: In 42_vgg the parameter n_channels is initilaized with value 16. It then says that ""the  following layers are the multiple of n_channels"". Can we hardcode the input/output channels as (1, 16), (16, 32), (32, 48), .... or will this trigger a hidden test in the autograder? Should we instead use e.g.n_channels  x 2, n_channels x 3 etc. ? (edited) "
"I have found something that may slow down the validation a lot. In the training of VGG the skip training is at the end of the epoch, so is the skip training is True the first epoch will be computed anyway."
"I'm getting an accuracy of 0.905 with VGG net, and 0.904 with ResNet. Is this alright?"
"In 41_lenet, I didn't get point for Test Cell 3 because in 01 email, it is said that ""it is expected that a batch normalization layer comes right after a convolutional layer"", so I add nn.BatchNorm2d(32) after nn.Conv2d(16, 32, 5) (which of course affect the expected value of y). Other than that, my Net seems to be the same. Could there be a chance to reconsider this?"
"In 41_lenet, the input to Net is a batch of 32 images (shape 32x...), and is expected to output class scores for each image in the batch (shape 32x10). So am I supposed to just iterate through the batch in forward() or is there some clever ""correct"" way to do this? (edited) "
"In 42_vgg, can we set the channels to 16, 32, etc. or do we need to use n_channels and multiple it? The code works, but I am worried that it triggers again some hidden tests"
"In 43_resnet I'm attempting to run the tests for the Block class. However, the assertion says that my y shape is bad after the second test where the number of channels is increased. I've printed out the shape of the output before it is returned and it is the same what the assertion uses (batch_size, 32, 28,28), but the tests says the second value of the shape is 16 instead of 32."
"In VGG style network, my loss increases and saturates at 2.~  from 2nd epoch.. 1st epoch runs fine.. error steadily reduces.  Dunno whats wrong. (edited) "
"In making the complete resnet, is it okay to do a resnet with static amount of blocks in the groups as instructed in the assignment or do you have to design it in a way that some hidden test would change the n_blocks = [2, 2, 2] to something else?"
"In part 3, ResNet the instructions say ""The number of channels in the second block should be double the number of channels in the first block, the number of channels in the third block should be four times the number of channels in the first block."" However, the attached figure seems to indicate that the second and third groups should have these numbers of channels, which seems to make more sense?"
"In the ResNet, should we assume that the n_blocks has always 3 elements (3 groups). Also, is the 0.906 test accuracy good enough? It says it should be around 90-91 but I’m just thinking if the implementation will be graded only based on the network accuracy? (edited) "
"In the first part, could you give any hint for calculating the required input size for the first linear layer of the convolutional network?"
"In the useful tutorial we are given (https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py), is the network correct? The input of the first linear layer mislead me quite a bit and it doesn't seem to even work if I test it."
"Is there a mistake in the lecture slides, regarding the convolutional layer (slide 50 onwards): When we apply the convolution layer, shouldn't the x and y dimensions reduce by kernel_size - 1? Now the dimensions stay at 28 after the first conv layer, but it should be 24? At least that is what happens in the visualization tool given on slide 32, and also in the exercises. (edited) "
"Is there any reason on the autograder why our lenet would have all zero activations on the final layer, when during training we reached the requisite accuracy? This seems like very strange behaviour, almost like the autograder did not set the weights correctly or something…"
"Okay so, in resnet block, everything else checks out but when both the channels and the resolution changes the end result will be a size mismatch when summing the 2 paths. Debugged this by printing ofc. Can't find a mistake. Any ideas?"
Question about the 43_resnet block. ''If either the resolution or the number of channels change...''. Number of channels change compared to what? Does it basicly mean that if         in_channels != out_channels           -> number of channels change=?
Sorry I did not see the previous post
What is the order of BN and Relu in ResNet block? The notebook does not specify it unambiguously. (edited) 
"hi, in the 42_vgg I didnt get points for the architecture_soft test cell. Especially in this test assert n_conv_layers == 9, ""Wrong number of convolutional layers ({:d})"".format(n_conv_layers). What exactly is being test in this case? Can I know in which part of the forward I got a wrong number of layers?"
"in 41 i got 0.869 accuracy on the last test, which is just under 0.87. Could this be due to randomness? I'm pretty sure all the blocks passed validation."
joined #ex4_convolutional along with 19 others.
"one thing I’m not sure: in a block with 3 convolutional layers with n output channels, we should, in the first layer of this block, “transform” the inputs in the n output channels, and then, in the other 2 layers of this block use n as input and output channels, right?"
"the cell where we print the resnet architecture,
# Let us print the architecture of the network
net
my output doesn't print my groups that I added although they are there. This isn't going to be problem right? Just making sure the output of this cell isn't used for grading or something."
works for me
"yes, there is no bias term in the skip connection"
