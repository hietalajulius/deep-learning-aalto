{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5786ad50229c4291c85d3daa0b4a3a0c",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5. Recurrent neural networks\n",
    "\n",
    "## Part 2. Mini-batch training of sequence-to-sequence model\n",
    "\n",
    "In the first part of exercise 5, we updated a sequence-to-sequence model using only one input-ouput sequence pair at a time. That procedure is slow because:\n",
    "* The update direction is computed using only one example and it is therefore noisy. One needs to use a small learning rate.\n",
    "* Using only one example in a mini-batch does not fully use the advantage of parallel processing\n",
    "\n",
    "One difficulty of mini-batch training of sequence-to-sequence is that sequences may have varying lengths and this has to be taken into account. In this exercise, we will implement such training using tools provided by PyTorch.\n",
    "\n",
    "## Learning goals of part 2\n",
    "\n",
    "* to learn PyTorch tools for batch processing of sequences with varying lengths\n",
    "* to learn how to write a custom `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6104dd384b4f6bcf991f534d32f7e771",
     "grade": false,
     "grade_id": "cell-67960098bbc35c29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# Select data directory\n",
    "import os\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    # course_data_dir = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c56a5b7b057506dfb624db3f38d973dd",
     "grade": false,
     "grade_id": "cell-15d07bc375fe29f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c00e8313607747f0e06cbacf0881d7f",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We use the same translation dataset as in the first part of Exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1735483997c198a58658234a4e221f9",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of input-output sequences:\n",
      "torch.Size([6, 1]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Translation data\n",
    "from data import TranslationDataset, SOS_token, EOS_token, MAX_LENGTH\n",
    "data_dir = os.path.join(course_data_dir, 'translation_data')\n",
    "trainset = TranslationDataset(path=data_dir, train=True)\n",
    "\n",
    "input_seq, output_seq = trainset[0]\n",
    "print('Shapes of input-output sequences:')\n",
    "print(input_seq.shape, output_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8495d8ff43aede3b6ec1dfb43a5b16e",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "Next we write a custom data loader which puts sequences of varying lengths in one tensor. We do so by using a custom `collate_fn` as explained [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Our collate function combines input sequences in one tensor with extra values filled with zeros. Note that for processing input sequences we are going to use [torch.nn.utils.rnn.PackedSequence](https://pytorch.org/docs/stable/nn.html?highlight=packedsequence#torch.nn.utils.rnn.PackedSequence) class which requires sequences to be sorted by their lengths.\n",
    "\n",
    "Similarly, the function combines output sequences in one tensor with extra values filled with zeros. Your task is to implement that.\n",
    "\n",
    "Note that:\n",
    "* the output sequences need not be sorted by their lengths, so we cannot use [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence)\n",
    "* you should keep the same order of sequences in the input and output tensors\n",
    "* the new tensors should have the same data type as input tensors. You can use, for example, function [torch.Tensor.new_full](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.new_full) to create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f43c8f0afa496e826f05189bca96415",
     "grade": false,
     "grade_id": "cell-c72d12f7bd122b9b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (input_seq, output_seq),\n",
    "      input_seq is Tensor([seq_length, 1])\n",
    "      output_seq is Tensor([seq_length, 1])\n",
    "\n",
    "    Returns:\n",
    "      input_seqs: Tensor of padded input sequences: [max_seq_length, batch_size, 1].\n",
    "      output_seqs: Tensor of padded output sequences: [max_seq_length, batch_size, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    # sort a list by sequence length (descending order) to use pack_padded_sequence\n",
    "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    input_seqs, output_seqs = zip(*list_of_samples)\n",
    "    input_seq_lengths = [len(seq) for seq in input_seqs]\n",
    "    output_seq_lengths = [len(seq) for seq in output_seqs]\n",
    "\n",
    "    padding_value = 0\n",
    "    \n",
    "    # Put all input sequences to one tensor, pad with padding_value\n",
    "    pad_input_seqs = pad_sequence(input_seqs, batch_first=False, padding_value=padding_value)\n",
    "    \n",
    "    # Put all output sequences to one tensor, pad with padding_value\n",
    "    # We cannot use pad_sequence because the output sequences are not necessarily sorted according to the lengths\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pad_output_seqs = pad_sequence(output_seqs, batch_first=False, padding_value=padding_value)\n",
    "    \n",
    "    return pad_input_seqs, input_seq_lengths, pad_output_seqs, output_seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36a14a07b2d12d2df7a4380df9d8c3fc",
     "grade": true,
     "grade_id": "collate",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences combined:\n",
      "tensor([[6., 1.],\n",
      "        [7., 2.],\n",
      "        [8., 0.]])\n",
      "Lengths: [3, 2]\n",
      "Output sequences combined:\n",
      "tensor([[ 9.,  3.],\n",
      "        [10.,  4.],\n",
      "        [ 0.,  5.]])\n",
      "Lengths: [2, 3]\n",
      "Shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Check how collate function combines some test sequences\n",
    "# Test with FloatTensors\n",
    "pairs = [\n",
    "    (torch.FloatTensor([1, 2]).view(-1, 1), torch.FloatTensor([3, 4, 5]).view(-1, 1)),\n",
    "    (torch.FloatTensor([6, 7, 8]).view(-1, 1), torch.FloatTensor([9, 10]).view(-1, 1)),\n",
    "]\n",
    "pad_input_seqs, input_seq_lengths, pad_output_seqs, output_seq_lengths = collate(pairs)\n",
    "assert pad_input_seqs.shape == torch.Size([3, 2, 1]), \"Bad shape of pad_input_seqs: {}\".format(pad_input_seqs.shape)\n",
    "assert pad_input_seqs.dtype == torch.float32\n",
    "assert pad_output_seqs.shape == torch.Size([3, 2, 1]), \"Bad shape of pad_output_seqs: {}\".format(pad_output_seqs.shape)\n",
    "assert pad_output_seqs.dtype == torch.float32\n",
    "print('Input sequences combined:')\n",
    "print(pad_input_seqs[:, :, 0])\n",
    "print('Lengths:', input_seq_lengths)\n",
    "print('Output sequences combined:')\n",
    "print(pad_output_seqs[:, :, 0])\n",
    "print('Lengths:', output_seq_lengths)\n",
    "\n",
    "# Test with LongTensors\n",
    "pairs = [\n",
    "    (torch.LongTensor([1, 2]).view(-1, 1), torch.LongTensor([3, 4, 5]).view(-1, 1)),\n",
    "    (torch.LongTensor([6, 7, 8]).view(-1, 1), torch.LongTensor([9, 10]).view(-1, 1)),\n",
    "]\n",
    "pad_input_seqs, input_seq_lengths, pad_output_seqs, output_seq_lengths = collate(pairs)\n",
    "assert pad_input_seqs.shape == torch.Size([3, 2, 1]), \"Bad shape of pad_input_seqs: {}\".format(pad_input_seqs.shape)\n",
    "assert pad_input_seqs.dtype == torch.long\n",
    "assert pad_output_seqs.shape == torch.Size([3, 2, 1]), \"Bad shape of pad_output_seqs: {}\".format(pad_output_seqs.shape)\n",
    "assert pad_output_seqs.dtype == torch.long\n",
    "print(\"Shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eb49bf5540acde0e7a612d940df89cb",
     "grade": false,
     "grade_id": "cell-b4fb631ef92b42cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(dataset=trainset,\n",
    "                         batch_size=64,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=collate,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e75abe5f3b9deeae191bf38e034034e",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "RNN units implemented in PyTorch such as nn.GRU or nn.LSTM support processing of sequences of varying lenghts. This is done by using function [`torch.nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) and [`torch.nn.utils.rnn.pad_packed_sequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence). Naturally, the `forward` function of the encoder must process the whole sequence.\n",
    "\n",
    "Your task is to implement the `forward` function of the encoder. It should process input sequences in the same way as the encoder from part 1. The difference is that it can process multiple sequences (batch size can be larger than 1).\n",
    "You should implement the following steps:\n",
    "* embed input sequences\n",
    "* pack input sequences using `pack_padded_sequence`\n",
    "* apply GRU computations to packed sequences obtained in the previous step\n",
    "* convert packed sequence of GRU outputs into padded representation with `pad_packed_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51387ac3db5efbfef6112f6de92e3e44",
     "grade": false,
     "grade_id": "def_encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dictionary_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs: Tensor [max_seq_length, batch_size, 1]\n",
    "          seq_lengths: list of sequence lengths\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "          outputs: Tensor [max_seq_length, batch_size, hidden_size]\n",
    "          hidden: Tensor [1, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        batch_size = len(pad_seqs[0,:,:])\n",
    "        embedded = self.embedding(pad_seqs.view(-1,batch_size))\n",
    "        packed = pack_padded_sequence(embedded, seq_lengths, batch_first = False)\n",
    "        #print('output', output)\n",
    "        #print('seq_lengths', seq_lengths)\n",
    "        #print('hidden', hidden)\n",
    "        \n",
    "        out, hidden = self.gru(packed, hidden)\n",
    "        padded,_ = pad_packed_sequence(out, batch_first=False, padding_value=0.0)\n",
    "        \n",
    "        #print('padded', padded.shape)\n",
    "        output = padded\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size=1, device=device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e031f3c6e66181111d3bb250698300c4",
     "grade": true,
     "grade_id": "encoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let's test your code\n",
    "hidden_size = 3\n",
    "encoder = Encoder(dictionary_size=5, hidden_size=hidden_size).to(device)\n",
    "\n",
    "max_seq_length = 4\n",
    "batch_size = 2\n",
    "hidden = encoder.init_hidden(batch_size=batch_size).to(device)\n",
    "pad_seqs = torch.zeros(max_seq_length, batch_size, 1, dtype=torch.int64)\n",
    "pad_seqs[:, 0, 0] = torch.tensor([1, 2, 3, 4])\n",
    "pad_seqs[:, 1, 0] = torch.tensor([2, 3, 0, 0])\n",
    "pad_seqs = pad_seqs.to(device)\n",
    "outputs, new_hidden = encoder.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "\n",
    "assert outputs.shape == torch.Size([4, batch_size, hidden_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape={}, expected={}\".format(\n",
    "        outputs.shape, torch.Size([4, batch_size, hidden_size]))\n",
    "assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), \\\n",
    "    \"Bad shape of outputs: new_hidden.shape={}, expected={}\".format(\n",
    "        new_hidden.shape, torch.Size([1, batch_size, hidden_size]))\n",
    "\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "054114726c903975025269a24b8568ef",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is similar to the one from part 1 of the exercise, except that it\n",
    "* processes multiple sequences at the same time\n",
    "* accepts padded target sequences\n",
    "\n",
    "Your task is to implement the same functionality as in the decoder of part 1. That is you need to implement the decoder with the following structure:\n",
    "<img src=\"seq2seq_decoder.png\" width=500 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to part 1:\n",
    "* Apply ReLU nonlinearities to the output word's embeddings (as shown in the figure).\n",
    "* We are going to use `nn.NLLLoss` loss for training, which accepts log-probabilities of the target words. Therefore, we need to apply `F.log_softmax` nonlinearity to produce log-probabilities.\n",
    "* Use a linear layer to map the states of GRU to the word logits (inputs of `F.log_softmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f97a91f10e96ad3ebbec6ca4e4d92f5",
     "grade": false,
     "grade_id": "cell-1c4dc8113a1680c2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_dictionary_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dictionary_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_dictionary_size)\n",
    "\n",
    "    def forward(self, hidden, pad_target_seqs=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden (tensor):          The state of the GRU (shape [1, batch_size, hidden_size])\n",
    "          pad_target_seqs (tensor): Tensor of words (word indices) of the target sentence. The shape is\n",
    "                                     [max_seq_length, batch_size, 1]. If None, the output sequence\n",
    "                                     is generated by feeding the decoder's outputs (teacher_forcing has to be False).\n",
    "\n",
    "        Returns:\n",
    "          outputs (tensor): Tensor of log-probabilities of words in the output language\n",
    "                             (shape [max_seq_length, batch_size, output_dictionary_size])\n",
    "          hidden (tensor):  New state of the GRU (shape [1, batch_size, hidden_size])\n",
    "        \"\"\"\n",
    "        if pad_target_seqs is None:\n",
    "            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'\n",
    "\n",
    "        batch_size = hidden.size(1)\n",
    "        prev_word = torch.tensor(SOS_token * np.ones((1, batch_size)), device=device, dtype=torch.int64)\n",
    "        max_length = pad_target_seqs.size(0) if pad_target_seqs is not None else MAX_LENGTH\n",
    "        outputs = []  # Collect outputs of the decoder at different steps in this list\n",
    "        for t in range(max_length):\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            output = self.embedding(prev_word).view(1,1,-1)\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(hidden, hidden)\n",
    "            output = F.log_softmax(self.out(output), dim= 2)\n",
    "            \n",
    "            outputs.append(output)\n",
    "\n",
    "            if teacher_forcing:\n",
    "                # Feed the target as the next input\n",
    "                prev_word = pad_target_seqs[t]\n",
    "            else:\n",
    "                # Use its own predictions as the next input\n",
    "                topv, topi = output[0, :].topk(1)\n",
    "                prev_word = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)  # [max_length, batch_size, output_dictionary_size]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "181ae79d65cfaa4aa423ca6c480f4721",
     "grade": true,
     "grade_id": "decoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "# Let's test the shapes\n",
    "hidden_size = 2\n",
    "output_dictionary_size = 5\n",
    "test_decoder = Decoder(hidden_size, output_dictionary_size).to(device)\n",
    "\n",
    "max_seq_length = 4\n",
    "batch_size = 2\n",
    "hidden = test_decoder.init_hidden(batch_size=batch_size, device=device)\n",
    "\n",
    "pad_target_seqs = torch.zeros(max_seq_length, batch_size, 1, dtype=torch.int64)\n",
    "pad_target_seqs[:, 0, 0] = torch.tensor([1, 2, 3, 4])\n",
    "pad_target_seqs[:, 1, 0] = torch.tensor([3, 2, 0, 0])\n",
    "pad_target_seqs = pad_target_seqs.to(device)\n",
    "\n",
    "outputs, new_hidden = test_decoder.forward(hidden, pad_target_seqs, teacher_forcing=False)\n",
    "assert outputs.size(0) <= 4, \"Too long output sequence: outputs.size(0)={}\".format(outputs.size(0))\n",
    "assert outputs.shape[1:] == torch.Size([batch_size, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape[1:]={}, expected={}\".format(\n",
    "        outputs.shape[1:], torch.Size([batch_size, output_dictionary_size]))\n",
    "assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), \\\n",
    "    \"Bad shape of new_hidden: new_hidden.shape={}, expected={}\".format(\n",
    "        new_hidden.shape, torch.Size([1, batch_size, hidden_size]))\n",
    "\n",
    "outputs, new_hidden = test_decoder.forward(hidden, pad_target_seqs, teacher_forcing=True)\n",
    "assert outputs.shape == torch.Size([4, batch_size, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape={}, expected={}\".format(\n",
    "        outputs.shape, torch.Size([4, batch_size, output_dictionary_size]))\n",
    "assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), \\\n",
    "    \"Bad shape of new_hidden: new_hidden.shape={}, expected={}\".format(\n",
    "        new_hidden.shape, torch.Size([1, batch_size, hidden_size]))\n",
    "\n",
    "# Generation mode\n",
    "outputs, new_hidden = test_decoder.forward(hidden, None, teacher_forcing=False)\n",
    "assert outputs.shape[1:] == torch.Size([batch_size, output_dictionary_size]), \\\n",
    "    \"Bad shape of outputs: outputs.shape[1:]={}, expected={}\".format(\n",
    "        outputs.shape[1:], torch.Size([batch_size, output_dictionary_size]))\n",
    "assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), \\\n",
    "    \"Bad shape of new_hidden: new_hidden.shape={}, expected={}\".format(\n",
    "        new_hidden.shape, torch.Size([1, batch_size, hidden_size]))\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56424019b5b99b40fe52d63d5ff2b8ef",
     "grade": false,
     "grade_id": "cell-4fee2ecfa669da47",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = Encoder(trainset.input_lang.n_words, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, trainset.output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2379eeacfdf8194fcf538dbd62d7c76c",
     "grade": false,
     "grade_id": "cell-a4c0ec87b04daf41",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Loss calculations\n",
    "\n",
    "In the training loop (see the code in the training section), the decoder produces a tensor of log-probabilities of words in the output language. We need to use these log-probabilities and the indices of the words in the target sequence to compute the loss. We are going to use [`torch.nn.NLLLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss) for that.\n",
    "\n",
    "Your task is to implement the loss computations in the cell below. You need to compute the average loss for all the words that appear in the **target** sequences:\n",
    "$$\n",
    "  c = \\frac{1}{n} \\sum_{l=1}^{n} \\text{loss}(\\mathbf{p}_l, t_l) .\n",
    "$$\n",
    "Here\n",
    "* $n=\\sum_k n_k$ is the total number of words in the target sequences ($n_k$ is the number of words in $k$-th target sequence).\n",
    "* $\\mathbf{p}_l$ is the vector of log-probabilities corresponding to $l$-th word,\n",
    "* $t_l$ is the index of the corresponding target word,\n",
    "* $\\text{loss}$ is implemented by `torch.nn.NLLLoss` (created at the beginning of the cell).\n",
    "\n",
    "Note that elements of `pad_target_seqs` equal to `padding_value` indicate elements which should be excluded from the cost computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ee4b0fbabe67a57ac9b156bafe667fd",
     "grade": false,
     "grade_id": "cell-640dbaa61dcc241c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(reduction='none')  # Use this criterion in the loss calculations\n",
    "\n",
    "def compute_loss(decoder_outputs, pad_target_seqs, padding_value=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      decoder_outputs (tensor): Tensor of log-probabilities of words produced by the decoder\n",
    "                                (shape [max_seq_length, batch_size, output_dictionary_size])\n",
    "      pad_target_seqs (tensor): Tensor of words (word indices) of the target sentence (padded with `padding_value`).\n",
    "                                 The shape is [max_seq_length, batch_size, 1]\n",
    "      padding_value (int):      Padding value. Keep the default one: the default padding value never\n",
    "                                 appears in real sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    for a, pad_target_seq in enumerate(pad_target_seqs):\n",
    "        for b, word in enumerate(pad_target_seq):\n",
    "            if word != padding_value:\n",
    "                #print('a', a)\n",
    "                #print('decoder_outputs[a]',decoder_outputs[a])\n",
    "                loss+= criterion(decoder_outputs[a,b].view(1,-1),word)\n",
    "                #print(loss)\n",
    "                n+=1\n",
    "    \n",
    "    loss = np.squeeze(loss)/n\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f14a51f88350cbe44081234168e18a54",
     "grade": true,
     "grade_id": "loss",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes seem to be ok.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_seq_length = 4\n",
    "pad_target_seqs = torch.zeros(max_seq_length, batch_size, 1, dtype=torch.int64)\n",
    "pad_target_seqs[:, 0, 0] = torch.tensor([1, 2, 3, 4])\n",
    "pad_target_seqs[:, 1, 0] = torch.tensor([3, 2, 0, 0])\n",
    "decoder_outputs = torch.zeros(max_seq_length, batch_size, 5)\n",
    "decoder_outputs[:, 0, [1, 2]] = 1\n",
    "decoder_outputs[:, 1, [2]] = 1\n",
    "loss = compute_loss(decoder_outputs, pad_target_seqs, padding_value=0)\n",
    "assert loss.shape == torch.Size([]), \"Bad shape of loss: {}\".format(loss.shape)\n",
    "print(\"The shapes seem to be ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c111980a7ee64bb425ea1bfee88b1065",
     "grade": false,
     "grade_id": "cell-6207d3c96c443b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Training of sequence-to-sequence model using mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24cdbcd925073aefdbb1f298be8908a9",
     "grade": false,
     "grade_id": "cell-6ce059e5ee375d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "loss_total = 0  # Reset every print_every\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.005)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40a3ac554e52ff47cf4d226fe8598cdd",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the training loop, we first encode input sequences using the encoder, then we decode the encoded state using the decoder. As we did previously in part 1, we sometimes use the target output sequence as the input of the decoder (teacher forcing) and sometimes feed the decoder's outputs as inputs (no teacher forcing). Naturally, the latter mode will be used during testing when no target sequence exists.\n",
    "\n",
    "The decoder outputs a tensor that contains probabilities of words in the output language. Your task is to use those probabilities to compute the loss. Note that you need to ignore the padded values in the output sequences caused by varying lengths of the output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1696b9f140d756ee1e471ada8e91425f",
     "grade": false,
     "grade_id": "cell-39d518fd8074b9ee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   136] loss: 2.2403\n",
      "[2,   136] loss: 1.7047\n",
      "[3,   136] loss: 1.4090\n",
      "[4,   136] loss: 1.1513\n",
      "[5,   136] loss: 0.9455\n",
      "[6,   136] loss: 0.7759\n",
      "[7,   136] loss: 0.6518\n",
      "[8,   136] loss: 0.5493\n",
      "[9,   136] loss: 0.4840\n",
      "[10,   136] loss: 0.4334\n",
      "[11,   136] loss: 0.3913\n",
      "[12,   136] loss: 0.3766\n",
      "[13,   136] loss: 0.3492\n",
      "[14,   136] loss: 0.3314\n",
      "[15,   136] loss: 0.3085\n",
      "[16,   136] loss: 0.3066\n",
      "[17,   136] loss: 0.2910\n",
      "[18,   136] loss: 0.2903\n",
      "[19,   136] loss: 0.2967\n",
      "[20,   136] loss: 0.2896\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 200  # iterations\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch\n",
    "        batch_size = pad_input_seqs.size(1)\n",
    "        pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "\n",
    "        encoder_hidden = encoder.init_hidden(batch_size, device)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Encode input sequence\n",
    "        _, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths, encoder_hidden)\n",
    "\n",
    "        # Decode using target sequence for teacher forcing\n",
    "        decoder_hidden = encoder_hidden\n",
    "        teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs, teacher_forcing=teacher_forcing)\n",
    "\n",
    "        # decoder_outputs is [max_seq_length, batch_size, output_dictionary_size]\n",
    "        # pad_target_seqs in [max_seq_length, batch_size, 1]\n",
    "        loss = compute_loss(decoder_outputs, pad_target_seqs, padding_value=0)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1) or i == (len(trainset) // trainloader.batch_size):\n",
    "            print('[%d, %5d] loss: %.4f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if skip_training:\n",
    "            break\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d9b054f16cceca3a03e4c887dbacf3e",
     "grade": false,
     "grade_id": "cell-61e2b32bce54d9c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that training proceeds much faster compared to the implementation in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0fa415ff09350748c8aa6238c159903",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 5_encoder_batch.pth, 5_decoder_batch.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk, submit these files together with your notebook\n",
    "encoder_filename = '5_encoder_batch.pth'\n",
    "decoder_filename = '5_decoder_batch.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(encoder.state_dict(), encoder_filename)\n",
    "            torch.save(decoder.state_dict(), decoder_filename)\n",
    "            print('Model saved to %s, %s.' % (encoder_filename, decoder_filename))\n",
    "        else:\n",
    "            print('Model not saved.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    hidden_size = 256\n",
    "    encoder = Encoder(trainset.input_lang.n_words, hidden_size)\n",
    "    encoder.load_state_dict(torch.load(encoder_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Encoder loaded from %s.' % encoder_filename)\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    decoder = Decoder(hidden_size, trainset.output_lang.n_words)\n",
    "    decoder.load_state_dict(torch.load(decoder_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Decoder loaded from %s.' % decoder_filename)\n",
    "    decoder = decoder.to(device)\n",
    "    decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7073eaa674ae067a57af177f25d60cca",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Below is the function that converts an input sequence to an output sequence using the trained sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "509004902a76367e9d860a3809615406",
     "grade": false,
     "grade_id": "cell-2122447b9917f9b6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_seq.size(0)\n",
    "        batch_size = 1\n",
    "\n",
    "        encoder_hidden = encoder.init_hidden(batch_size, device)\n",
    "        input_seq = input_seq.view(-1, 1, 1).to(device)\n",
    "        encoder_output, encoder_hidden = encoder(input_seq, [input_length], encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs=None, teacher_forcing=False)\n",
    "\n",
    "        output_seq = []\n",
    "        for t in range(decoder_outputs.size(0)):\n",
    "            topv, topi = decoder_outputs[t].data.topk(1)\n",
    "            output_seq.append(topi.item())\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c08bde1751c32102dd179b9c0484c2ec",
     "grade": false,
     "grade_id": "cell-fe6996ecee284354",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on training data:\n",
      "-----------------------------\n",
      "> nous avons toutes deux tort . EOS\n",
      "= we re both wrong . EOS\n",
      "< we re both wrong . EOS\n",
      "\n",
      "> il est genereux envers ses amis . EOS\n",
      "= he is generous to his friends . EOS\n",
      "< he is generous to his friends . EOS\n",
      "\n",
      "> il est tres amical avec nous . EOS\n",
      "= he is very friendly to us . EOS\n",
      "< he s very friendly to us . EOS\n",
      "\n",
      "> elle m a vole mes vetements ! EOS\n",
      "= she stole my clothes ! EOS\n",
      "< she stole my ! ! EOS\n",
      "\n",
      "> je ne suis pas un menteur . EOS\n",
      "= i m no liar . EOS\n",
      "< i m not liar . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate random sentences from the training set\n",
    "print('\\nEvaluate on training data:')\n",
    "print('-----------------------------')\n",
    "for i in range(5):\n",
    "    input_sentence, target_sentence = trainset[np.random.choice(len(trainset))]\n",
    "    print('>', ' '.join(trainset.input_lang.index2word[i.item()] for i in input_sentence))\n",
    "    print('=', ' '.join(trainset.output_lang.index2word[i.item()] for i in target_sentence))\n",
    "    output_sentence = evaluate(input_sentence)\n",
    "    print('<', ' '.join(trainset.output_lang.index2word[i] for i in output_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e3fc975713622f47e1cc1d7b070ce06",
     "grade": false,
     "grade_id": "cell-e27f5e4329673f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate random sentences from the test set\n",
    "testset = TranslationDataset(path=data_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90bd8570d7e376b348f696429114f0e6",
     "grade": false,
     "grade_id": "cell-c1cafaf3ca027d6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on test data:\n",
      "-----------------------------\n",
      "> c est un acteur . EOS\n",
      "= he is an actor . EOS\n",
      "< he s a grouch . EOS\n",
      "\n",
      "> il est occupe a apprendre l anglais . EOS\n",
      "= he is busy learning english . EOS\n",
      "< he is busy at at clever university . EOS\n",
      "\n",
      "> elles ont termine . EOS\n",
      "= they re done . EOS\n",
      "< they re done . EOS\n",
      "\n",
      "> je suis mecontent . EOS\n",
      "= i m unhappy . EOS\n",
      "< i m being . . EOS\n",
      "\n",
      "> je n en suis pas si sur ! EOS\n",
      "= i m not so sure . EOS\n",
      "< i m not sure ! ! EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluate on test data:')\n",
    "print('-----------------------------')\n",
    "for i in range(5):\n",
    "    input_sentence, target_sentence = testset[np.random.choice(len(testset))]\n",
    "    print('>', ' '.join(testset.input_lang.index2word[i.item()] for i in input_sentence))\n",
    "    print('=', ' '.join(testset.output_lang.index2word[i.item()] for i in target_sentence))\n",
    "    output_sentence = evaluate(input_sentence)\n",
    "    print('<', ' '.join(testset.output_lang.index2word[i] for i in output_sentence))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
