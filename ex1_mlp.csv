0
"0.01 seems to give slightly better results. So, this is the recommended value."
"@Alexander Ilin Hey, in Ex-1 we are suppose to submit a ouput network file along with the notebook,file named ""1_mlp.pth"", are we suppose to submit this through mycourses or through Jupyter hub?"
"After training the model locally and uploading the "".pth"" file to JupyterHub, I get the following message when I try to open it:"
"Am I supposed to use softmax on the outputs? Or should the last line of MLP.forward be just x = self.fc3(x)? If I use softmax, I get much worse results (loss doesn't go below 0.4) (edited) "
Awesome thanks!
Because I found this one
Currently unable to fetch the assignments in JupyterHub. Can it be done manually?
"Fixed to  Tuesday, 5 March, 06:00. My bad"
"Hello, I missing one point where should contact the course staff? Is here the right place?"
"Hello.

What is the difference between F.relu() and nn.ReLU()? The PyTorch documentation seems to use both, but I didn't figure out yet what is the difference in practical use. Thanks!"
"Hi everyone, Just a basic question, to start this assignment, and to it in colab, which drive are we suppose to save the file in? Because it just says Download the assignment files to your computer. Keep the same folder structure as in /notebooks/deeplearn2019/"
Hi! For some reason I am not able to see my results from the first assignment. It should be in deeplearning2019 folder? (edited) 
Hi! Shouldn't the training curve be smoother in the beginning?
"Hi, I am trying to run the script with GPU, but I keep getting the following error"
"Hi, I was checking the feedback for my first assignment and for some reason it seems that the data files have not been there and the tests have failed because of 'FileNotFoundError: File b'../data/winequality/winequality-red.csv does not exist'. Consequently, this results in more errors as the training data is missing. Everything worked without any errors when I was running it before the submission. Should I have submitted the data files separetely or something?"
"Hi, could I confirm the deadline for the 1st assignment"
"Hi, could I confirm the deadline for the 1st assignment? Because I found this one."
I couldn't find details on this sort of errors on Google.
I didn't get any feedback?
"I don't know if anybody else noticed, but it seems that the exercises expect you to define the layers in __init__(). I happened to do this in forward() in FancyMLP(). Doing it this way does not show the layers in final print call.

I'm not sure if this is counted as a mistake, since the function seems to accomplish what it's meant to do, but there might be autograding test that expect otherwise."
I get a very weird error in the first assignments training loop part. Would it be possible to get some help debugging it?
"I think I really misunderstood the “units” thing of the layer. I lost half of the points from the assignment because from one hidden layer to another (in the “multilayer perceptron (MLP) network” part), where the relu is applied, I didnt use a linear activation (nn.Linear(100, 100) according to the feedback tests). How should I know that I should use such linear function? If the relu keeps the dimensionality, shouldnt it also keep the units?"
"I understand the mlp.parameters are the gradients.
When task is to zero the gradients I was thinking to zero mlp instance.
However, in many examples the actually the optimiser zeroed.
On the other hand optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)
So the result should be the same (and practically it is), but the mlp was a lot faster to run.
So, what is the better way torch-wise of doing this?"
"Im getting this message when trying to validate my notebook, was there a way to re-fetch the assignment?"
"In ""Training loop"" section optimizer's learning rate is stated to be 0.01, however in the code optimizer is defined as:
optimizer = torch.optim.Adam(mlp.parameters(), lr=0.005)
Which one I should use?"
In one of the linked tutorials https://pytorch.org/tutorials/beginner/pytorch_with_examples.html a network with one hidden layer with ReLU activation function is created with nn.sequential by having a linear layer defining the input and hidden layer size and then nn.ReLU() with no arguments. When building the model using a self created constructor method does the input layer also need to be specifically defined or is input layer thought of as just the vector and the input weights to the hidden layer?
"In the first exercise, does a ""ReLU layer"" of size N mean a torch.nn.Linear layer of size N followed by a call to the ReLu activation function?"
"It may be a silly question, but what does ""100 units"" mean when defining the hidden layers, in the first net of the assignment?"
"It should not matter, you can try both and see which one gives the best training error. Sorry for confusion."
"Just to make sure that I have understood everything correctly:

I can not be sure that my submission is correctly done, or how many points I will be awarded. ""validate"" with skip_training=true only runs all the cells, even if it says that ""notebook passed all the tests"". These are not the tests of the autograder, and for example the correctness of FancyMLP I have to test myself. Is this correct?

In addition, I'm feeling a bit confused with the non-editable commented second cell, which says that ""in evaluation, this cell sets skip_training=true"", even if it is commented. What is meant by evaluation here? Running the cell, validating the notebook or the work done by autograder / course personnel?"
"March 5, 6:00 AM (edited) "
"Maybe it's a silly question, but how can I properly transform x_train_scaled and y_train, because when I use torch.from_numpy() function, I get this error?"
"My program seems to run perfectly fine with ""skip_training"" set to False (accuracy plot makes sense etc). It also saves the network to a file (1_mlp.pth) as it should. Then, if I set skip_training to True, all still seems fine as long as all variables are still in memory. However, if I restart the Jupyter kernel and clear the variables from memory, and still use skip_training=True, then the saved network information is not loaded properly: the accuracy plot is empty and the confusion matrix for MLP is a copy of the confusion matrix for the random forest classifier. It does say ""Model loaded from 1_mlp.pth"", though. Any ideas?"
"Please come to one of the exercise session, one is happening now in 182, Maarintalo, Sähkömiehentie 3"
"Regarding the FancyMLP forward, the link gives the example line x = self.linears[i](x) where self.linears[i] is layer to be computed with x. I can’t figure out how to parametrisize the activation function that the x is passed to and used in this layer. Or have I misunderstood it completely? Besides the example line does work, python understands the line as tuple: “TypeError: 'tuple' object cannot be interpreted as an integer”"
"Similarly, in the FancyMLP exercise, should each of the layer types be torch.nn.Linear followed by specified activation function or what should the layer type in each of the layers be?"
"So, in the FancyMLP class, the init method has the activation_fn as an argument whereas the forward method doesn't. Isn't the activation_fn supposed to be used in the forward method? In the init method, we just define the layers right? I had to hardcode the activation_fn when I wanted to use it in the forward method. Is that okay?"
Submitting the .pth file: That is not visible in Assingments when submitting when it is in that folder. It should not? You will find it from there?
Thanks very much.
The deadline is 4th or 5th ?
"The plots are for your information. They are not evaluated. With skip_training=True, the accuracy plot is empty. And yes, there is that problem with the second confusion matrix. It is also not evaluated."
Trouble with SMB? You can also use scp between different unix machines. In Jupyter click New dropdown menu and choose Terminal. That opens a terminal window in the exercise forder and you can copy files from home or Aalto computers with scp command.
We were supposed to change the lr in the Adam optimizer right? There is no # YOUR CODE HERE…
What is the proper way to come and discuss the about feedback?
When is the deadline for the first exercise?
"When saving the torch file in JupyterHub, you get a file with Error! /notebooks/deeplearn2019/1_mlp/1_mlp.pth is not UTF-8 encoded"
joined #ex1_mlp along with 18 others.
joined #ex1_mlp.
renamed the channel from “ex1” to “ex1_mlp”
set the channel purpose: Discussion related to the 1st exercise
"through Jupyter hub, copy the files there and run your notebook with skip_training=True to make sure that everything works"
